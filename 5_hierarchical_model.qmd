---
editor: visual
execute:
  warning: false
---

# Hierarchical Model

```{r}
# Load libraries
library(tidyverse)
library(dagitty)
library(ggdag)
library(GGally)
library(ggpmisc)
library(gt)
library(cmdstanr)
library(tidybayes)
library(ggridges)
library(loo)
#library(posterior)

# Set ggplot theme
theme_set(theme_classic())

# Create a function to convert decimals to percentages
to_percent <- function(x, digits = 0){
  return(round(x * 100, digits = digits))
}

# Custom lower panel with regression line, equation, and styled points
lower_fun <- function(data, mapping, ...) {
  ggplot(data = data, mapping = mapping) +
    geom_point(shape = 1, alpha = 0.4, size = 1.5) +
    geom_smooth(method = "lm", formula = 'y ~ x', se = FALSE, color = "steelblue") +
    geom_abline(intercept = 0, slope = 1, linetype = "dashed") +
    stat_poly_eq(
      formula = y ~ x,
      aes(label = after_stat(eq.label)),
      parse = TRUE,
      size = 3
    ) +
    scale_x_continuous(labels = scales::percent, breaks = seq(-1,1, 0.25)) +
    scale_y_continuous(labels = scales::percent, breaks = seq(-1,1, 0.250))
}
```

It's sensible to assume that the teacher has a systematic impact on student achievement for two reasons. First, they are often the ones grading students and some teachers can be more lenient or harsher graders than others. Second, some teachers may be better or worse than others at teaching which would impact performance on tests. Given that it's easy to collect information on who the teacher was, let's add it to our DAG as seen in @fig-dag-hierarchical.

```{r}
#| label: fig-dag-hierarchical
#| fig-cap: "Hierarchical DAG with the effect of the teacher."

dag <- dagitty('dag {
bb="0,0,1,1"
teacher [exposure,pos="0.5,0.8"]
math_8 [exposure,pos="0.3,0.3"]
mth1w [exposure,pos="0.500,0.5"]
mpm2d [outcome,pos="0.700,0.5"]
teacher -> math_8
teacher -> mth1w
teacher -> mpm2d
math_8 -> mth1w
math_8 -> mpm2d
mth1w -> mpm2d
}
')

ggdag_status(dag, node_size = 20) +
  guides(color = "none") +  # Turn off legend
  theme_dag()
```
## Mathematical Model

$$\text{logit}(\mu_{8i}) = \tau_{8j} + \alpha_8$$

$$\phi_8, \phi_9, \phi_{10} \sim N^+(10, 3)$$

$$x_{8i} \sim \text{Beta}(\mu_{8i} \cdot \phi_8, \, (1 - \mu_{8i}) \cdot \phi_8))$$
$$\text{logit}(\mu_{9i}) = \tau_{9j} + \alpha_9 + \beta_1 \cdot (x_{8i} - \bar{x}_8)$$
$$x_{9i} \sim \text{Beta}(\mu_{9i} \cdot \phi_9,\ (1 - \mu_{9i}) \cdot \phi_9)$$
$$\text{logit}(\mu_{10i}) = \tau_{10j} + \alpha_{10} + \beta_2 \cdot (x_{8i} - \bar{x}_8) + \beta_3 \cdot (x_{9i} - \bar{x}_9)$$
$$y_{10i} \sim \text{Beta}(\mu_{10i} \cdot \phi_{10},\ (1 - \mu_{10i}) \cdot \phi_{10})$$
Let's try to write some reasonable priors for the $\alpha$ and $\tau$ parameters. Alpha represents the expected log-odds grade for the average student. In the previous chapters we assumed that the average student would finish with 80%, 77%, and 72% in year 8, 9, 10 respectively. Thus, we could set the following priors. 

```{r}
tibble(
  x = seq(0.95, 1.4, 0.05), 
  y = plogis(x)
)
```

```{r}
#| label: fig-alpha-prior-h
#| fig-cap: "Visualizing the log-odds to set plausible priors."

ggplot() +
  stat_function(
    fun = plogis,
    xlim = c(-3, 3)
  ) +
  geom_vline(xintercept = 1.4) +
  scale_x_continuous(breaks = -6:6) +
  scale_y_continuous(breaks = seq(-1, 1, 0.1), labels = scales::percent) +
  labs(
    x = "Alpha",
    y = "Average Grade"
  )
```

```{r}
ggplot() +
  stat_function(
    fun = dnorm,
    args = list(mean = 1.4, sd = 0.03),
    xlim = c(0.8, 1.5)
  ) +
  stat_function(
    fun = dnorm,
    args = list(mean = 1.2, sd = 0.03),
    xlim = c(0.8, 1.5)
  ) +
  stat_function(
    fun = dnorm,
    args = list(mean = 0.95, sd = 0.03),
    xlim = c(0.8, 1.5)
  ) +
  scale_x_continuous(breaks = seq(0, 5, 0.1))+
  labs(
    x = "Alpha",
    y = "Density"
  )
```

$$\alpha_8 \sim N(1.4, 0.03)$$
$$\alpha_9 \sim N(1.2, 0.03)$$
$$\alpha_{10} \sim N(0.95, 0.03)$$
Now let's pivot to focus on the $\tau$ parameters which represent the teacher effects.We know some teacher's are harsher than others but we can assume than the average teacher is a relatively grader. In other words, we assume that teacher bias is a random error of measurement instead of a systematic one. It's entirely possible that teachers are systematically too generous in their grading because they want their students to succeed. 

As seen in @fig-alpha-prior-h a one unit increase in log-odds represents roughly a 20% difference in student grades. We don't expect teacher effects to be this strong so we might set the following prior.

$$\tau_{8j}, \tau_{9j}, \tau_{10j} \sim N(0, 0.3)$$
```{r}
ggplot() +
  stat_function(
    fun = dnorm,
    args = list(mean = 0, sd = 0.3),
    xlim = c(-1, 1)
  ) +
    labs(
    x = "Teacher Effect on log-odds",
    y = "Density"
  )
```
## Simulated Teacher Data

Let's simulate some data based on our mathematical model and our priors.

```{r}
N <- 250

teacher_8 <- rep(
  c("A", "B", "C", "C", "C", "D", "D", "D", "D", "D"),
  times = c(19, 29, 23, 30, 27, 22, 25, 28, 18, 29)
)

teacher_9 <- rep(
  c("E", "E", "E", "E", "F", "F", "F", "F", "G", "G"),
  times = c(21, 27, 31, 20, 22, 26, 25, 28, 24, 26)
)

teacher_10 <- rep(
  c("F", "F", "F", "G", "G", "G", "G", "G", "H", "H"),
  times = c(27, 24, 29, 28, 22, 20, 23, 26, 26, 25)
)

set.seed(2025)
teachers <- tibble(
  student = 1:N,
  teacher_8 = sample(teacher_8),
  bias_8 = case_when(
    teacher_8 == "A" ~ -0.1,
    teacher_8 == "B" ~ 0,
    teacher_8 == "C" ~ -0.3,
    teacher_8 == "D" ~ 0.25
  ),
  teacher_9 = sample(teacher_9),
  bias_9 = case_when(
    teacher_9 == "E" ~ 0,
    teacher_9 == "F" ~ -0.5,
    teacher_9 == "G" ~ 0.2
  ),
  teacher_10 = sample(teacher_10),
  bias_10 = case_when(
    teacher_10 == "F" ~ -0.3,
    teacher_10 == "G" ~ 0.2,
    teacher_10 == "H" ~ 0.4
  )
)
```

```{r}
set.seed(2025)
sim_grades <- teachers |>
  mutate(
    mu8 = plogis(1.39 + bias_8),
    y8 = rbeta(n = N, shape1 = mu8*10, shape2 = (1-mu8)*10),
    mu9 = plogis(1.21 + bias_9 + 5 * (y8 - mean(y8))),
    y9 = rbeta(n = N, shape1 = mu9 * 10, shape2 = (1 - mu9) * 10),
    mu10 = plogis(
      0.94 + bias_10 + 1.2 * (y8 - mean(y8)) + 4.35 * (y9 - mean(y9))
      ),
    y10 = rbeta(n = N, shape1 = mu10 * 10, shape2 = (1 - mu10) * 10),
)
```

```{r}
sim_grades |>
  pivot_longer(
    cols = starts_with("y"),
    names_to = "year", 
    values_to = "grade"
  ) |>
  mutate(
    year = factor(
      year,
      levels = c("y8", "y9", "y10"),
      labels = c("year 8", "MTH1W", "MPM2D")
      )) |>
  ggplot(aes(x = grade, fill = year)) +
  geom_density(alpha = 0.2) +
  geom_vline(xintercept = 0.5, linetype = "dashed") +
  scale_x_continuous(breaks = seq(0, 1, 0.1),labels = scales::percent) +
  labs(
    x = "Simulated Grades",
    y = "Probability Density"
  )

sim_grades |>
  select(starts_with("y")) |>
  ggpairs(lower = list(continuous = lower_fun))

sim_grades |>
  summary()
```

```{r}
sim_grades_by_teacher <- sim_grades |>
  pivot_longer(
    cols = starts_with("y"),
    names_to = "student_grade",
    names_prefix = "y",
    values_to = "result",
  ) |>
  pivot_longer(
    cols = starts_with("teacher"),
    names_to = "teacher_grade",
    names_prefix = "teacher_",
    values_to = "teacher",
  ) |>
  pivot_longer(
    cols = starts_with("bias"),
    names_to = "bias_grade",
    names_prefix = "bias_",
    values_to = "bias",
  ) |>
  filter(student_grade == teacher_grade, student_grade == bias_grade) |>
  mutate(
    teacher_grade = factor(teacher_grade, levels = c("8", "9", "10")),
    teacher = factor(teacher, levels = c("A", "B", "C", "D", "E", "F", "G", "H"))
  )

sim_grades_by_teacher |>
  group_by(teacher_grade) |>
  summarise(
    n = n(),
    mean_bias = mean(bias),
    mean = mean(result),
    median = median(result),
    sd = sd(result)
  ) |>
  gt() |>
  fmt_number(columns = where(is.numeric), decimals = 2)

sim_grades_by_teacher |>
  group_by(teacher) |>
  summarise(
    n = n(),
    mean_bias = mean(bias),
    mean = mean(result),
    median = median(result),
    sd = sd(result)
  ) |>
  gt() |>
  fmt_number(columns = where(is.numeric), decimals = 2)

sim_grades_by_teacher |>
  group_by(teacher_grade, teacher) |>
  summarise(
    n = n(),
    mean_bias = mean(bias),
    mean = mean(result),
    median = median(result),
    sd = sd(result)
  ) |>
  gt() |>
  fmt_number(columns = where(is.numeric), decimals = 2)
```

We see that our data is realistic and that teacher effects do indeed shift the distributions. Let's focus our attention on the three teachers in year 9.

```{r}
sim_grades_by_teacher |>
  filter(student_grade == 9) |>
  ggplot(aes(x = result, fill = teacher)) +
  geom_density(alpha = 0.5) +
    stat_function(
    fun = dbeta,
    args = list(shape1 = 0.77*10, shape2 = (1-0.77)*10),
    xlim = c(0,1),
    size = 3, color = "darkorange"
  ) +
  geom_vline(xintercept = 0.5, linetype = "dashed") +
  scale_x_continuous(breaks = seq(0, 1, 0.1),labels = scales::percent) +
  labs(
    title = "Bias: E = 0, F = -0.5, G = 0.2",,
    subtitle = "The orange line is the theoretical density",
    x = "Simulated Year 9 Grades",
    y = "Probability Density"
  )
```

We see that the green distribution is shifted to the left relative to the orange curve because of the negative bias of -0.5 on the log-odd scale. Conversely, the blue distribution is slightly shifted to the right while the red distribution roughly follows the theoretical distribution.

## Consistent Teacher Effects

The previous simulation assumed that the teacher effects were independent between years. That is, a harsh grader in year 9 was no more likely to be a grader in year 10 than a easy grader. It seems reasonable to assume that some teachers are systematically harsher teachers than others across grades and that their bias may differ for different courses.

$$
\zeta_j \sim \mathcal{N}(0, \sigma_\zeta)
$$

$$
\tau_{8j} \sim \mathcal{N}(\zeta_j, \sigma_8)
$$

$$
\tau_{9j} \sim \mathcal{N}(\zeta_j, \sigma_9)
$$

$$
\tau_{10j} \sim \mathcal{N}(\zeta_j, \sigma_{10})
$$

$$
\phi_8, \phi_9, \phi_{10} \sim \mathcal{N}^+(10, 3)
$$

$$
\text{logit}(\mu_{8i}) = \tau_{8j[i]} + \alpha_8
$$

$$
x_{8i} \sim \text{Beta}(\mu_{8i} \cdot \phi_8,\ (1 - \mu_{8i}) \cdot \phi_8)
$$

$$
\text{logit}(\mu_{9i}) = \tau_{9j[i]} + \alpha_9 + \beta_1 \cdot (x_{8i} - \bar{x}_8)
$$

$$
x_{9i} \sim \text{Beta}(\mu_{9i} \cdot \phi_9,\ (1 - \mu_{9i}) \cdot \phi_9)
$$

$$
\text{logit}(\mu_{10i}) = \tau_{10j[i]} + \alpha_{10} + \beta_2 \cdot (x_{8i} - \bar{x}_8) + \beta_3 \cdot (x_{9i} - \bar{x}_9)
$$

$$
y_{10i} \sim \text{Beta}(\mu_{10i} \cdot \phi_{10},\ (1 - \mu_{10i}) \cdot \phi_{10})
$$


```{r}
set.seed(2025)

teachers <- tibble(
  student = 1:N,
  teacher_8 = sample(teacher_8),
  teacher_9 = sample(teacher_9),
  teacher_10 = sample(teacher_10)
)

unique_teachers <- teachers |>
  pivot_longer(
    cols = starts_with("teacher"),
    names_to = "teacher_grade",
    names_prefix = "teacher_",
    values_to = "teacher",
  ) |>
  distinct(teacher) |>
  arrange(teacher) |>
  mutate(
    teacher_id = row_number()
  )

K <- nrow(unique_teachers)

set.seed(2025)
unique_teachers <- unique_teachers |>
  arrange(teacher) |>
  mutate(zeta = rnorm(K, 0, 0.3))
 
unique_teachers_grades <- teachers |>
  pivot_longer(
    cols = starts_with("teacher"),
    names_to = "teacher_grade",
    names_prefix = "teacher_",
    values_to = "teacher",
  ) |>
  distinct(teacher_grade, teacher) |>
  arrange(teacher_grade, teacher) |>
  inner_join(unique_teachers, by = join_by(teacher)) 

L <- nrow(unique_teachers_grades)

set.seed(2025)
unique_teachers_grades <- unique_teachers_grades|>
  mutate(
    tau = rnorm(n= L, mean = zeta, sd = 0.1)
  )
 
unique_teachers_grades

teachers_with_effects <- teachers |>
  pivot_longer(
    cols = starts_with("teacher_"),
    names_to = "teacher_grade",
    names_prefix = "teacher_",
    values_to = "teacher"
  ) |>
  mutate(teacher_grade = as.character(teacher_grade)) |>
  left_join(unique_teachers_grades, by = join_by(teacher_grade, teacher)) |>
  pivot_wider(
    id_cols = student,
    names_from = teacher_grade,
    values_from = c(teacher, zeta, tau),
    names_sep = "_"
  )

teachers_with_effects

set.seed(2025)

sim_grades <- teachers_with_effects |>
  mutate(
    mu8 = plogis(1.39 + tau_8),
    y8 = rbeta(n = N, shape1 = mu8*10, shape2 = (1-mu8)*10),
    mu9 = plogis(1.21 + tau_9 + 5 * (y8 - mean(y8))),
    y9 = rbeta(n = N, shape1 = mu9 * 10, shape2 = (1 - mu9) * 10),
    mu10 = plogis(
      0.94 + tau_10 + 1.2 * (y8 - mean(y8)) + 4.35 * (y9 - mean(y9))
      ),
    y10 = rbeta(n = N, shape1 = mu10 * 10, shape2 = (1 - mu10) * 10),
)
```

```{r}
sim_grades |>
  pivot_longer(
    cols = starts_with("y"),
    names_to = "year", 
    values_to = "grade"
  ) |>
  mutate(
    year = factor(
      year,
      levels = c("y8", "y9", "y10"),
      labels = c("year 8", "MTH1W", "MPM2D")
      )) |>
  ggplot(aes(x = grade, fill = year)) +
  geom_density(alpha = 0.2) +
  geom_vline(xintercept = 0.5, linetype = "dashed") +
  scale_x_continuous(breaks = seq(0, 1, 0.1),labels = scales::percent) +
  labs(
    x = "Simulated Grades",
    y = "Probability Density"
  )

sim_grades |>
  select(starts_with("y")) |>
  ggpairs(lower = list(continuous = lower_fun))

sim_grades |>
  summary()
```


```{r}
sim_grades_by_teacher <- sim_grades |>
  pivot_longer(
    cols = starts_with("y"),
    names_to = "student_grade",
    names_prefix = "y",
    values_to = "result",
  ) |>
  pivot_longer(
    cols = starts_with("teacher"),
    names_to = "teacher_grade",
    names_prefix = "teacher_",
    values_to = "teacher",
  ) |>
  pivot_longer(
    cols = starts_with("tau"),
    names_to = "bias_grade",
    names_prefix = "tau_",
    values_to = "bias",
  ) |>
  filter(student_grade == teacher_grade, student_grade == bias_grade) |>
  mutate(
    teacher_grade = factor(teacher_grade, levels = c("8", "9", "10")),
    teacher = factor(teacher, levels = c("A", "B", "C", "D", "E", "F", "G", "H"))
  )

sim_grades_by_teacher |>
  group_by(teacher_grade) |>
  summarise(
    n = n(),
    mean_bias = mean(bias),
    mean = mean(result),
    median = median(result),
    sd = sd(result)
  ) |>
  gt() |>
  fmt_number(columns = where(is.numeric), decimals = 2)

sim_grades_by_teacher |>
  group_by(teacher) |>
  summarise(
    n = n(),
    mean_bias = mean(bias),
    mean = mean(result),
    median = median(result),
    sd = sd(result)
  ) |>
  gt() |>
  fmt_number(columns = where(is.numeric), decimals = 2)

sim_grades_by_teacher |>
  group_by(teacher_grade, teacher) |>
  summarise(
    n = n(),
    mean_bias = mean(bias),
    mean = mean(result),
    median = median(result),
    sd = sd(result)
  ) |>
  gt() |>
  fmt_number(columns = where(is.numeric), decimals = 2)
```

## Stan Hierarchical Model

```{r}
sim_grades <- sim_grades |>
  left_join(unique_teachers, by = c("teacher_8" = "teacher")) |>
  rename(teacher_8_id = teacher_id) |>
  left_join(unique_teachers, by = c("teacher_9" = "teacher")) |>
  rename(teacher_9_id = teacher_id) |>
  left_join(unique_teachers, by = c("teacher_10" = "teacher")) |>
  rename(teacher_10_id = teacher_id)

# sim_grades |>
#   select(
#     student,
#     starts_with("teacher_"),
#     starts_with("y")
#   ) |>
#   write_csv(file = "data/sim_grades_teacher.csv")
```


```{r}
data_list <- list(
  N = nrow(sim_grades),
  J = sim_grades |>
  select(teacher_8, teacher_9, teacher_10) |>
  unlist() |>
  unique() |>
  length(),
  teacher_8 = sim_grades$teacher_8_id,
  teacher_9 = sim_grades$teacher_9_id,
  teacher_10 = sim_grades$teacher_10_id,
  x8 = sim_grades$y8,
  x9 = sim_grades$y9,
  y10 = sim_grades$y10
)
```

```{r}
#| eval: FALSE

# Compile the Stan Model
teacher_mod <- cmdstan_model(stan_file = 'stan_models/teacher_model.stan')

# Fit the model
fit_teacher_mod <- teacher_mod$sample(
  dat = data_list,
  seed = 2025
  )

# Save the Stan model
fit_teacher_mod$save_object(file = "stan_models/fit_teacher_mod.RDS")

fit_teacher_mod$cmdstan_diagnose()

rm("teacher_mod", "fit_teacher_mod")
```

```{r}
# Load the Stan model
fit_teacher_mod <- readRDS("stan_models/fit_teacher_mod.RDS")
```

```{r}
fit_teacher_mod$diagnostic_summary()

fit_teacher_mod$sampler_diagnostics(format = "df")
```

```{r}
#| label: tbl-s
#| tbl-cap: "Summary Statistics of Posterior Parameters"

fit_teacher_mod$summary(
  variables = c(
    "alpha8", "alpha9", "alpha10",
    "phi8", "phi9", "phi10",
    "beta1", "beta2", "beta3",
    "zeta",
    "tau_8", "tau_9", "tau_10")
  ) |>
  gt() |>
  fmt_number(columns = where(is.numeric), decimals = 2)
```

```{r}
post_long <- fit_teacher_mod |>
  spread_draws(y10_hat[student]) |>
  inner_join(sim_grades, by = join_by(student))

grades_pred_summary <- post_long |>
  group_by(student) |>
  summarise(
    n = n(),
    y8 = first(y8),
    y9 = first(y9),
    y10 = first(y10),
    y10_hat_mean = mean(y10_hat),
    y10_hat_median = median(y10_hat),
    residuals = y10 - y10_hat_median,
    y10_hat_sd = sd(y10_hat),
    lower_95 = quantile(y10_hat, 0.025),
    upper_95 = quantile(y10_hat, 0.975),
    p_fail = mean(y10_hat < 0.5),
    p_1 = mean(y10_hat >= 0.5 & y10_hat < 0.6),
    p_2 = mean(y10_hat >= 0.6 & y10_hat < 0.7),
    p_3 = mean(y10_hat >= 0.7 & y10_hat < 0.8),
    p_4 = mean(y10_hat >= 0.8),
    p_1_om = mean(y10_hat >= 0.5),
    p_2_om = mean(y10_hat >= 0.6),
    p_3_om = mean(y10_hat >= 0.7),
    p_4_om = mean(y10_hat >= 0.8),
    p_pass = mean(y10_hat >= 0.5)
  )

post_long <- post_long |>
  inner_join(grades_pred_summary, by = join_by(student, y8, y9, y10))

# Randomly select 25 student IDs
set.seed(2025)
random_25_students <- grades_pred_summary |>
  slice_sample(n = 25) |>
  pull(student)
```


```{r, fig.height=10, fig.width=8}
#| label: fig-ridges-teacher
#| fig-cap: "geom_density_ridges_gradient()"

post_long |>
  filter(student %in% random_25_students) |>
  ggplot(aes(x = y10_hat, y = fct_reorder(factor(student), y10_hat_mean))) +
  geom_density_ridges_gradient(
    aes(height = ..density.., fill = ..x..),
    stat = "density",
    scale = 0.95,
    rel_min_height = 0.01
  ) +
  stat_pointinterval(.width = c(0.66, 0.90), point_interval = median_hdi) +
  geom_point(aes(x = y10), color = "blue", size = 2) +
  geom_vline(xintercept = 0.5, linetype = "dashed") +
  scale_x_continuous(
    breaks = seq(0, 1, 0.1), limits = c(0, 1),
    labels = scales::percent
  ) +
  scale_fill_gradientn(
    colours = c("red", "red", "yellow", "lightgreen", "forestgreen"),
    values = c(0, 0.45, 0.6, 0.8, 1),
    limits = c(0, 1),
    guide = "none"
  ) +
  labs(
    x = "Posterior Predicted Year 10 Result",
    y = "Student",
    title = "Posterior Predictive Distributions for 25 Random Students",
    subtitle = "66% and 90% median HDI ; Blue point = true observed value"
  ) +
  theme(
    panel.grid.major = element_line(color = "gray90"),
    panel.grid.minor = element_blank(),
    panel.grid.major.x = element_line(color = "gray90"),
    panel.grid.major.y = element_blank(),
  )

#ggsave("posterior_plot_teacher.png", width = 8, height = 10)
```


```{r}
fit_teacher_mod$loo()
```


```{r}
#| eval: FALSE
linear_mod <- cmdstan_model(stan_file = 'linear_model.stan')
fit_linear_mod <- linear_mod$sample(
  dat = data_list
  )

logit_mod <- cmdstan_model(stan_file = 'logit_model.stan')

fit_logit_mod <- logit_mod$sample(
  dat = data_list
  )

loo_compare(x = list(
  linear_model = fit_linear_mod$loo(),
  logit_model = fit_logit_mod$loo(),
  teacher_model = fit_teacher_mod$loo()
))
```


